{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_Test.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QN-F6cYDOfSM"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SB_-LKKCOqHK"
      },
      "source": [
        "import pandas as pd \n",
        "data = pd.read_pickle(\"/content/gdrive/MyDrive/TICNN_Implementation/TICNN/final_text_df.pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKPmrrvpOuQP"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "#Progress bars\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "#Paralelize pandas apply on multiple cores\n",
        "#import swifter\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import style\n",
        "#Nicer style\n",
        "style.use('seaborn')\n",
        "import seaborn as sns\n",
        "\n",
        "from tensorflow import keras as k\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEe2gep7Ox4f"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "df=pd.DataFrame()\n",
        "data[\"text\"] = (pad_sequences(data[\"text\"], maxlen=400,padding=\"post\", truncating=\"post\")).tolist()\n",
        "data[\"title\"] = pad_sequences(data[\"title\"], maxlen=400,padding=\"post\", truncating=\"post\").tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4cBpiGAO0um"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data[['text', 'title']], data['type'], test_size=0.3, random_state=1)\n",
        "\n",
        "#Train - valid\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X_train, y_train, test_size=0.3, random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uQqwNI1O3EA"
      },
      "source": [
        "X_train_title = X_train['title']\n",
        "X_train_content = X_train['text']\n",
        "\n",
        "X_valid_title = X_valid['title']\n",
        "X_valid_content = X_valid['text']\n",
        "\n",
        "X_test_title = X_test['title']\n",
        "X_test_content = X_test['text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_SuuZbSO5Am"
      },
      "source": [
        "train_fit = [np.asarray(X_train_title.tolist()), \n",
        "             np.asarray(X_train_content.tolist())]\n",
        "\n",
        "valid_fit = [np.asarray(X_valid_title.tolist()), \n",
        "             np.asarray(X_valid_content.tolist())]\n",
        "\n",
        "test_fit = [np.asarray(X_test_title.tolist()), \n",
        "             np.asarray(X_test_content.tolist())]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtkXhWLpO5n5"
      },
      "source": [
        "model  = k.models.load_model(\"path to H5 File\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPcI4HkyPItg"
      },
      "source": [
        "results = model.predict(test_fit)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hpbdwte5PK5O"
      },
      "source": [
        "results = (results>=0.5)*1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjCabu6PPM5u"
      },
      "source": [
        "cm=confusion_matrix(y_test,results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xz2teeOcPO3x"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.clf()\n",
        "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Wistia)\n",
        "classNames = ['Real','Fake']\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "tick_marks = np.arange(len(classNames))\n",
        "plt.xticks(tick_marks, classNames, rotation=45)\n",
        "plt.yticks(tick_marks, classNames)\n",
        "s = [['TN','FP'], ['FN', 'TP']]\n",
        "for i in range(2):\n",
        "    for j in range(2):\n",
        "        plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDsaOR1rPRJy"
      },
      "source": [
        "from sklearn.metrics import precision_score,recall_score,f1_score,accuracy_score\n",
        "precision = precision_score(y_test, results)\n",
        "# recall: tp / (tp + fn)\n",
        "recall = recall_score(y_test,results)\n",
        "# f1: 2 tp / (2 tp + fp + fn)\n",
        "f1 = f1_score(y_test, results)\n",
        "print('Precision is:'+str(precision))\n",
        "print('Recall is:'+str(recall))\n",
        "print('F1 score is:'+str(f1))\n",
        "print('Accuracy is:'+str(accuracy_score(y_test,results)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}